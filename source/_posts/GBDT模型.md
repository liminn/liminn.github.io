---
title: GBDT模型
mathjax: true
top: true
date: 2017-12-13 21:20:50
categories: 
- 机器学习
tags:
- 提升树
- 梯度提升树
- GBDT
---
　　**提升(Boosting)方法**实际采用加法模型（即基函数的线性组合）与前向分布算法。
　　**提升树(boosting tree)**：以**决策树为基函数**的**提升方法**称为提升树。对分类问题，决策树是二叉分类树，对回归问题，决策树是二叉回归树。 
　　本文先记录回归问题的提升树算法，后记录回归问题的梯度提升树(gradient boosting decision tree,GBDT)算法。
<!-- more --> 
# 一、提升树
## 1. 回归问题的提升树算法
### 1.1 基本分类器：回归树 (同[CART回归树](http://zhanglimin.com/2017/12/07/CART%E6%A8%A1%E5%9E%8B/))
　　已知一个训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,$x_i \in X \in R^n$，$X$为输入空间，$y_i\in Y \in R$，$Y$为输出空间。如果将输入空间$X$划分为$J$个互不相交的区域$R_1,R_2,...,R_j$，并且在每个区域上确定输出的常量$c_j$，那么树可表示为
$$T\left(x;\varTheta\right)=\sum_{j=1}^Jc_jI(x\in R_j)$$
　　其中，参数$\varTheta=\{(R_1,c_1),(R_2,c_2),...,(R_J,c_J)\}$表示树的区域划分和各区域上的常数。$J$是回归树的复杂度即叶结点个数。
### 1.2 提升树模型
　　提升树模型可以表示为决策树的加法模型：
$$f_M(x)=\sum_{m=1}^{M}T(x;\theta_m)$$
　　其中，$T(x;\Theta_m)$表示决策树；$\Theta_m$为决策树的参数；$M$为树的个数。
### 1.3 代价函数：平方误差损失函数
$$L(y,f(x))=(y-f(x))^2$$
### 1.4 学习算法：前向分步算法
回归问题提升树使用以下前向分布算法：
$$
\begin{eqnarray*}
f_0(x)&=&0\\
f_m(x)&=&f_{m-1}(x)+T(x;\Theta_m), \quad m=1,2,...,M\\
f_M(x)&=&\sum_{m=1}^{M}T(x;\theta_m)
\end{eqnarray*}
$$
在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解
$$\hat{\Theta}_m=arg\min_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$
得到$\hat{\Theta}_m$，即第$m$颗树的参数。
当采用平方误差损失函数时，
$$L(y,f(x))=(y-f(x))^2$$
其损失变为
$$L(y,f_{m-1}(x)+T(x;\Theta_m))\\
=[(y-f_{m-1}(x)-T(x;\Theta_m))]^2\\
=[r-T(x;\Theta_m)]^2$$
这里
$$r=y-f_{m-1}(x)$$
是当前模型拟合数据的残差。所以，**对回归问题的提升树算法来说，只需要简单地拟合当前模型的残差**。
### 1.5 回归问题的提升树算法
输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$, $x_i \in X \in R^n$，$y_i\in Y \in R$
输出：提升树$f_M(x)$
（1）初始化$f_0(x)=0$
（2）对$m=1,2,...,M$
（a）计算残差
$$r_{mi}=y_i−f_{m−1}(x_i),\ i=1,2,...,N$$
（b）拟合残差`$r_{mi}$`学习一个回归树，得到 `$T(x;\Theta_m)$`
（c）更新`$f_m(x)=f_{m−1}(x)+T(x;\Theta_m)$`
（3）得到回归问题提升树
$$f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$$

### 1.6 例题
训练数据见下表，$x$的取值范围为区间$[0.5,10.5]$,$y$的取值范围为区间$[5.0,10.0]$,学习这个回归问题的最小二叉回归树。

|$x_i$| 1|	2	|3	|4	|5	|6	|7	|8	|9	|10|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|$y_i$|	5.56|	5.70|	5.91|	6.40|	6.80|	7.05|	8.90|	8.70|	9.00|	9.05|

首先来看这个优化问题
$$\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$
求解训练数据的切分点$s$:
$$R_1=\lbrace x\mid x\le s\rbrace , \quad R_2=\lbrace x\mid x\gt s\rbrace$$
容易求得在$R_1$,$R_2$内部使得平方损失误差达到最小值的$c_1$,$c_2$为：
$$c_1={1\over N_1}\sum_{x_i \in R_1}y_i , \quad c_2={1\over N_2}\sum_{x_i \in R_2}y_i$$
这里$N_1$,$N_2$是$R_1$,$R_2$的样本点数。
求训练数据的切分点，根据所给数据，考虑如下切分点：
$$1.5 ,2.5 ,3.5 ,4.5 ,5.5 , 6.5 , 7.5 , 8.5 , 9.5$$
对各切分点，不难求出相应的$R1$ , $R2$ , $c1$ , $c2$及
$$m(s)=\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$
例如，当$s=1.5$时，$R_1 = \lbrace 1\rbrace$, $R_2 = \lbrace 2, 3 , \ldots , 10\rbrace$ , $c_1 = 5.56$ , $c_2 = 7.50$ ,
$$m(s)=\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2] = 0+15.72 = 15.72$$
现将$s$及$m(s)$的计算结果列表如下：

|$s$| 	1.5|	2.5|	3.5|	4.5|	5.5|	6.5|	7.5|	8.5|	9.5|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|$m(s)$|	15.72|	12.07|	8.36|	5.78|	3.91|	1.93|	8.01|	11.73|	15.74|

由上表可知，当$s=6.5$的时候达到最小值，此时$R_1 = \lbrace 1 ,2 , \ldots , 6\rbrace$, $R_2 ={7 ,8 ,9 , 10}$ , $c_1=6.24$, $c_2=8.9$, 所以回归树$T_1(x)$为：
$$T_1(x) =
\begin{cases}
6.24, & x\lt 6.5 \\
8.91, & x \ge 6.5 \\
\end{cases}$$
$$f_1(x) = T_1(x)$$
用`$f_1(x)$`拟合训练数据的残差见下表，表中`$r_{2i} = y_i - f_1(x_i),i=1,2,\ldots , 10$`

|$x_i$| 	1|	2|	3|	4|	5|	6|	7|	8|	9|	10|
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|$y_i$|	-0.68|	-0.54|	-0.33|	0.16|	0.56|	0.81|	-0.01|	-0.21|	0.09|	0.14|

用$f_1(x)$拟合训练数据的平方误差：
$$L(y,f_1(x)) = \sum_{i=1}^{10}(y_i-f_1(x_i))^2 = 1.93$$
第2步求$T_2(x)$.方法与求$T_1(x)$一样，只是拟合的数据是上表的残差，可以得到：
$$T_2(x) =
\begin{cases}
-0.52, & x\lt 3.5 \\
0.22, & x \ge 3.5 \\
\end{cases}$$
$$f_2(x) = f_1(x) + T_2(x)=
\begin{cases}
5.72, & x\lt 3.5 \\
6.46, & 3.5\le x \lt 6.5 \\
9.13, & x\ge 6.5 \\
\end{cases}$$
用$f_2(x)$拟合训练数据的平方误差是：
$$L(y,f_2(x)) = \sum_{i=1}^{10}(y_i-f_2(x_i))^2 = 0.79$$
继续求得
$$T_3(x) =
\begin{cases}
0.15, & x\lt 6.5 \\
-0.22, & x \ge 6.5 \\
\end{cases}
\quad L(y,f_3(x)) = 0.47 ,$$
$$T_4(x) =
\begin{cases}
-0.16, & x\lt 4.5 \\
0.11, & x \ge 4.5 \\
\end{cases}
\quad L(y,f_3(x)) = 0.30 ,$$
$$T_5(x) =
\begin{cases}
0.07, & x\lt 6.5 \\
-0.11, & x \ge 6.5 \\
\end{cases}
\quad L(y,f_3(x)) = 0.23 ,$$
$$T_6(x) =
\begin{cases}
-0.15, & x\lt 2.5 \\
0.04, & x \ge 2.5 \\
\end{cases}$$
$$f_6(x) = f_5(x)+T_6(x) =T_1(x)+ \ldots + T_5(x) + T_6(x)=
\begin{cases}
5.63, & x\lt 2.5 \\
5.82, & 2.5 \le x\lt 3.5 \\
6.56, & 3.5 \le x\lt 4.5 \\
6.83, & 4.5 \le x\lt 6.5 \\
8.95, & x\ge 6.5 \\
\end{cases}$$
用$f_6(x)$拟合训练数据的平方损失误差是
$$L(y,f_6(x)) = \sum_{i=1}^{10}(y_i-f_6(x_i))^2 = 0.17$$
假设此时已经满足误差要求，那么$f(x) = f_6(x)$即为所求的回归树。

## 2.分类问题的提升树算法
　　对于二分类问题，提升树算法只需将[AdaBoost算法](http://zhanglimin.com/2017/12/13/Adaboost%E7%AE%97%E6%B3%95/)中的基本分类器限制为二类分类树即可(如[CART分类树](http://zhanglimin.com/2017/12/07/CART%E7%AE%97%E6%B3%95/))。
　　故略。

# 二、梯度提升树决策模型
　　提升树利用加法模型与前向分布算法实现学习的优化过程。**当损失函数是平方损失函数时，每一步优化是很简单的，只需简单地拟合当前模型的残差**；**但对于一般损失函数而言，往往每一步优化并不那么容易**，针对这一问题，梯度提升算法(gradient boosting)被提出。
## 2.1 核心思想
　　**用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值**，拟合一个回归树： 
$$-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}$$
## 2.2 梯度提升算法
**输入**：训练数据集 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$,$x_i \in X \in R^n$，$y_i\in Y \in R$，损失函数$L(y,f(x))$
**输出**：回归树$\hat{f}(x)$
**步骤1**：初始化
$$f_0\left(x\right)=arg\min_c\sum_{i=1}^N{L\left(y_i,c\right)}$$
**说明**：算法第一步初始化，估计使损失函数极小化的常数值，它是只有一个根结点的树。
**步骤2**：对 $m=1,2,...,M$
（a）对 $i=1,2,...,N$，计算
$$r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}$$
**说明**：计算损失函数的负梯度在当前模型的值，将它作为残差的估计。**对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值**。
（b）对`$r_{mi}$`拟合一个回归树，得到第$m$颗树的叶结点区域`$R_{mj}$`，`$j=1,2,...,J$`
**说明**：估计回归树叶结点区域，以拟合残差的近似值。
（c）对`$j=1,2,...,J$`，计算
$$c_{mj}=arg\min_c\sum_{x_i\in R_{mj}}{L\left(y_i,f_{m-1}\left(x_i\right)+c\right)}$$
**说明**：利用线性搜索估计叶结点区域的值，使损失函数极小化。
（d）更新
$$f_m\left(x\right)=f_{m-1}\left(x\right)+\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}$$
**说明**：更新回归树。
**步骤3**：得到回归树
$$\hat{f}\left(x\right)=f_M\left(x\right)=\sum_{m=1}^M{\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}}$$
**说明**：得到输出的最终模型 $\hat{f}(x)$

## 2.3 GBDT的特点（待补）
GBDT中的树是回归树，不是分类树；
优点：GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。
RF与GBDT对比：
（1）RF中树的棵树是并行生成的；GBDT中树是顺序生成的；两者中过多的树都会过拟合，但是GBDT更容易过拟合；
（2）RF中每棵树分裂的特征比较随机；GBDT中前面的树优先分裂对大部分样本区分的特征，后面的树分裂对小部分样本区分特征；
（3）RF中主要参数是树的棵数；GBDT中主要参数是树的深度，一般为1；

# 三、参考
- 李航，统计学习方法
- [AndDS, GBDT](http://aandds.com/blog/ensemble-gbdt.html)


















