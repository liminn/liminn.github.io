---
title: 逻辑回归模型
date: 2017-12-4 16:35:50
mathjax: true
top: true
categories: 
- 机器学习
tags:
- 逻辑回归
- Logistic Regression
- sigmoid函数
- 似然估计
- 梯度下降
- 决策边界
- 正则化
---
逻辑回归(logistic regression)属于监督学习中的分类(classification)模型，模型输出为离散值。

## 1.模型
### 1.1 sigmoid函数
在介绍逻辑回归模型之前，先引入sigmoid函数，其数学形式是：
$$g(x) = \frac{1}{1 + e ^ {-x}}$$
对应的函数曲线如下图所示：
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/sigmoid.png" width="30%" height="30%">
sigmoid函数是一个S形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0/1（这个性质使我们能够以概率的方式来解释）。
<!-- more --> 
### 1.2 决策函数
**假设函数**：
$$h_\theta(x)= P(y=1|x;\theta)=g(\theta^T x) = \frac{1}{1 + e ^ {-\theta^T x}}$$
其中，$x=[x_0,x_1,...,x_n]^T\in R^{n+1}$, $x_0^{(i)}=1$(对应偏置项); $\theta = [\theta_0,\theta_1,...,\theta_n]^T\in R^{n+1}$

<!-- more --> 
**假设函数解释**：$h_\theta(x)=P(y=1|x;\theta)$，即**$h_\theta(x)$意为在给定输入为$x$，参数为$\theta$的条件下，$y=1$的概率**。此外，$P(y=0|x;\theta)=1-P(y=1|x;\theta)$。
**相应的决策函数为**：
$$y^* = 1, \, \textrm{if} \, \ h_\theta(x) > 0.5$$
一般选0.5作为为阈值，实际应用时特定的情况可以选择不同阈值，例如对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。

### 1.3 代价函数
对于训练集：$\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$，其中$y^{(i)}\in \{ 0,1 \} $
（1）最大似然估计
模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然估计，即找到一组参数，使得在这组参数下，数据的似然度（概率）越大。在逻辑回归模型中，似然度可表示为：
$$L(\theta) = P(D|\theta) = \prod_{i=1}^m P(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^m g(\theta^T x^{(i)}) ^ {y^{(i)}} (1-g(\theta^T x^{(i)}))^{1-y{(i)}}$$
取对数可以得到对数似然度：
$$l(\theta) = logL(\theta)=\sum_{i=1}^m {y^{(i)}\log{g(\theta^T x^{(i)})} + (1-y^{(i)})\log{(1-g(\theta^T x^{(i)}))}}$$
（2）代价函数
另一方面，在机器学习领域，更经常遇到的是损失函数的概念，其衡量的是模型预测错误的程度。常用的损失函数有0-1损失，log损失，hinge损失等。其中log损失在单个数据点上的定义为：
$$-y\log{h_\theta(x)}-(1-y)\log{(1-h_\theta(x))}$$
如果取整个数据集上的平均log损失，我们可以得到：
$$J(\theta) = -\frac{1}{m} l(\theta)=-\frac{1}{m} \sum_{i=1}^m {y^{(i)}\log{h_\theta(x^{(i)})} + (1-y^{(i)})\log{(1-h_\theta(x^{(i)}))}}$$
即在逻辑回归模型中，我们最大化似然函数和最小化log损失函数实际上是等价的。
### 1.4 参数求解
对于该优化问题，常用的凸优化的方法都可以用于求解该问题，例如梯度下降，共轭梯度下降，BFGS，LBFGS等，这里以梯度下降的为例说明。
逻辑回归的代价函数是凸函数，梯度下降总是收敛到全局最小值。
梯度下降(Gradient Descent)又叫作最速梯度下降，是一种迭代求解的方法，通过在每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。
基本步骤如下：

- 选择下降方向（负梯度方向，$\nabla {J(\theta)}$)
- 选择步长$\alpha$，更新参数 $\theta^i = \theta^{i-1} - \alpha^i \nabla {J(\theta^{i-1})}$
- 重复以上两步直到满足终止条件


<img src="http://ozruihqgo.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/gradient_descent.png" width="30%" height="30%">
其中代价的梯度计算方法为：
$$\frac{\partial{}}{\partial{\theta_j}}J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y^{(i)}-h_\theta (x^{(i)}) )x^{(i)}_j $$
计算过程：
先看如何对sigmoid函数求导：

$$
\begin{align*}
\sigma(x)'&=&\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}\\
&=&\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2}\\&=&\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)\\&=&\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)=\sigma(x)(1 - \sigma(x))
\end{align*}
$$

利用上面的结果，借助复合函数求导公式等，可得：
$$
\begin{align*}
\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)}))   + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j  \newline&= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j
\end{align*} 
$$

### 1.5 决策边界
进行求解参数后，可得模型的输出结果。决策函数为$y^* = 1, \, \textrm{if} \, \ h_\theta(x) > 0.5$，可从sigmoid函数看出，当$\theta^T x > 0$时，$y=1$，否则 $y=0$。$\theta^T x = 0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面），即决策边界。所以说逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。下面两个图的对比说明了线性分类曲线和非线性分类曲线（通过特征映射）。
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E9%AB%98%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE.jpg" width="70%" height="70%">
左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是在特征转换$[x_1, x_2] => [x_1, x_2, x_1^2, x_2^2, x_1x_2]$ 后的空间是线性可分的，对应的原始空间中分类边界为一条类椭圆曲线。
### 1.6 正则化
当模型的参数过多时，很容易遇到过拟合的问题。这时就需要有一种方法来控制模型的复杂度，典型的做法在优化目标中加入正则项，通过惩罚过大的参数来防止过拟合：
$$J(\theta) = -\frac{1}{m}\sum {y\log{g(\theta^T x)} + (1-y)\log{(1-g(\theta^T x))}} + \lambda \Vert \theta \Vert_p$$
一般情况下，取$p=1$或$p=2$，分别对应L1，L2正则化，两者的区别可以从下图中看出来，L1正则化（左图）倾向于使参数变为0，因此能产生稀疏解。
L2正则化：
$$J(\theta) =-\frac{1}{m} \sum_{i=1}^m {y^{(i)}\log{h_\theta(x^{(i)})} + (1-y^{(i)})\log{(1-h_\theta(x^{(i)}))}}+\lambda||\theta||^2_2$$
L1正则化：
$$J(\theta) =-\frac{1}{m} \sum_{i=1}^m {y^{(i)}\log{h_\theta(x^{(i)})} + (1-y^{(i)})\log{(1-h_\theta(x^{(i)}))}}+\lambda||\theta||_1$$
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96.jpg" width="70%" height="70%">
实际应用时，由于数据的维度可能非常高，L1正则化因为能产生稀疏解，使用的更为广泛一些。

## 2.多分类
如果$y$不是在[0,1]中取值，而是在$K$个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当$K$个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果$K$个类别是互斥的，即 $y=i$ 的时候意味着 $y$ 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。
（1）One-vs-all

- 对每一类别$i$训练一个逻辑回归分类器$h_\theta^{(i)}(x)$，来预测$y=i$的可能性。
- 对于一个新的输入，要做出预测，选择$h_\theta^{(i)}(x)$最大的那个，即$\max\limits_ih_\theta^{(i)}(x)$

（2）[Softmax回归](http://zhanglimin.com/2017/12/04/Softmax%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/)
Softmax回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。
模型通过 softmax 函数来对概率建模，具体形式如下：
$$P(y=i|x, \theta) = \frac{e^{\theta_i^T x}}{\sum_j^K{e^{\theta_j^T x}}}$$
而决策函数为：$y^* = \textrm{argmax}_i P(y=i|x,\theta)$
对应的损失函数为：
$$J(\theta) = -\frac{1}{m} \sum_i^m \sum_j^K {1[y_i=j] \log{\frac{e^{\theta_i^T x}}{\sum {e^{\theta_k^T x}}}}}$$
类似的，也可以通过梯度下降或其他高阶方法来求解该问题，这里不再赘述。

注：

- 疑问1：到底什么是交叉熵
- 疑问2：逻辑回归的损失函数和softmax的损失函数有什么联系，如何推导出这个联系？

## 3.参考资料
- [Andrew Ng, Machine Learning](https://www.coursera.org/learn/machine-learning)
- [林轩田，机器学习基石](https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf)
- 周志华，机器学习
- [美团技术点评团队，Logistic Regression 模型简介](https://tech.meituan.com/intro_to_logistic_regression.html)
- [UFLDL Tutorial, Softmax回归](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)

