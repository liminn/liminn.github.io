---
title: 朴素贝叶斯分类器
mathjax: true
date: 2017-12-8 21:41:50
top: true
categories: 
- 机器学习
tags:
---
　　朴素贝叶斯法(naive Bayes)是基于贝叶斯定理与特征条件独立假设的分类方法。
# 一、概念
1.贝叶斯公式
$$P(\theta|x)=\frac{P(\theta)P(x|\theta)}{P(x)}\tag{1}$$
　　式中，$x$为观察到的数据，$\theta$为决定数据分布的参数，$P(\theta|x)$为后验概率(posterior)，$P(x)$为迹象(evidence)，$P(\theta)$为先验概率(prior)，$P(x|\theta)$为似然估计(likelihood)。
<!-- more --> 
2.特征条件独立假设
　　特征条件独立假设意为：用于分类的特征在类确定的情况下都是条件独立的。
$$P(x|c) = \prod\limits_{i=1}^dP(x_i|c)\tag{2}$$
　　式中，$d$为特征数目，$x_i$为$x$在第$i$个属性上的取值。
# 二、朴素贝叶斯分类器
　　基于贝叶斯定理，$P(c|x)$可写为：
$$P(c|x)=\frac{P(c)P(x|c)}{P(x)}\tag{3}$$
　　不难发现，基于贝叶斯公式来估计后验概率$P(c|x)$的主要困难在于：**类条件概率$P(x|c)$是所有属性上的联合概率，难以从有限的训练样本直接估计而得**。
　　**注**：条件概率分布$P(x|c)$有指数级数量的参数，其估计实际是不可行的。假设$$x_j$$可取值有$$S_j$$个，$$j=1,2,...,n$$，$y$可取值有$K$个，那么参数个数为$$K\prod_{j=1}^n S_j$$。
　　为避开这个障碍，朴素贝叶斯分类器采用了“**特征条件独立性假设**”，基于特征条件独立性假设，上式可重写为：
$$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod\limits_{i=1}^dP(x_i|c)\tag{4}$$
　　最终，**朴素贝叶斯分类器表达式**：
$$h_{nb}(x)=\mathop{argmax}\limits_{c\in{y}} P(c)\prod\limits_{i=1}^dP(x_i|c)\tag{5}$$
# 三、极大似然估计
　　显然，朴素贝叶斯分类器的训练(学习)过程就是基于训练集$D$来估计**类先验概率**$P(c)$，并为**每个属性估计条件概率**$P(x_i|c)$。
　　令$D_c$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分部样本，则**类先验概率**的极大似然估计是
$$P(c) = \frac{|D_c|}{|D|}\tag{6}$$
　　对于**离散属性**而言，令$D_{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则**条件概率**$P(x_i|c)$的极大似然估计是
$$P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}\tag{7}$$
　　对于**连续属性**可考虑概率密度函数,假定`$p(x_i|c)\sim N(\mu_{c,i},\sigma_{c,i}^2)$`,其中`$\mu_{c,i}$`和`$\sigma_{c,i}^2$`分别是第$c$类样本在第$i$个属性上取值的均值和方差，则有
$$P(x_i|c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x-\mu_{c,i})^2}{2\sigma_{c,i}^2})\tag{8}$$
# 四、贝叶斯估计
　　用极大似然估计可能会出现所要估计的概率值为0的情况。为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，即遇到$P(x_i|c)=0$的情况，这会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用**贝叶斯估计**(Bayesian estimation)。
　　具体来说，令$K$表示训练集$D$中可能出现的类别数，$S_i$表示第$i$个属性可能的取值数，先验概率和条件概率的贝叶斯估计是
$$
\begin{eqnarray*}
\hat{P}(c) &=& \frac{|D_c|+\lambda}{|D|+K \lambda}\tag{9}\\
\hat{P}(x_i|c)&=&\frac{|D_{c,x_i}|+\lambda}{|D_c|+S_i \lambda}\tag{10}
\end{eqnarray*}
$$
式中$\lambda \geq 0$。等价于在随机变量各个取值的频数上赋予一个正数$\lambda>0$。当$\lambda=0$时，就是极大似然估计。常取$\lambda=1$,这时称为拉普拉斯平滑(Laplace smoothing)。则式$(9)$和$(10)$分别修正为：
$$
\begin{eqnarray*}
\hat{P}(c) &=& \frac{|D_c|+1}{|D|+K}\tag{9}\\
\hat{P}(x_i|c)&=&\frac{|D_{c,x_i}|+1}{|D_c|+S_i}\tag{10}
\end{eqnarray*}
$$
ps:做完《机器学习》和《统计学习方法》中贝叶斯分类器一章的例题，可全部理解。
# 五、参考资料
- 周志华，机器学习
- 李航，统计学习方法
- [朴素贝叶斯分类器](https://wizardforcel.gitbooks.io/dm-algo-top10/content/naive-bayes.html)
- [分类算法之朴素贝叶斯分类](http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html)
- [先验分布、后验分布、似然估计](https://www.zhihu.com/question/24261751/answer/158547500)