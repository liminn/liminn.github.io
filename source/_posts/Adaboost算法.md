---
title: AdaBoost算法
mathjax: true
top: true
date: 2017-12-13 9:12:50
categories: 
- 机器学习
tags:
- Boosting
- AdaBoost
---
## 一、集成学习
　　**集成学习**(ensemble learning)通过构建并结合多个学习器来完成学习任务。根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，一类为个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表为**Boosting**；一类为个体学习器间不存在强依赖关系、可同时生成的并行化方法，代表为**Bagging和随机森林**。
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/%E6%A0%91%E7%9A%84%E5%8E%86%E5%8F%B2.JPG" width="70%" height="70%"> 　　Boosting是一族可将弱学习器提升为强学习器的算法。Boosting族算法最著名的代表是AdaBoost。本文主要记录AdaBoost算法。
<!-- more --> 
## 二、AdaBoost简介
　　（1）AdaBoost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）；
　　（2）AdaBoost在每一轮**如何改变训练数据的权值或概率分布**以及**如何将弱分类器组合成一个强分类器**的做法如下：
- 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值，这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注；
- AdaBoost采用加权多数表决的方法，具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。

## 三、AdaBoost算法
　　现叙述AdaBoost算法。假设给定一个二分类的训练数据集
$$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$
　　其中，每个样本点由实例与标记组成。实例$x_i\in X\in R^n$，标记`$y_i\in Y=\{-1,+1\}$`，$X$是实例空间，$Y$是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。
**算法（AdaBoost）**
**输入**：训练数据集$T$；弱学习算法
**输出**：最终分类器$G(x)$
**步骤1**.初始化训练数据的权值分布：
$$D_1=(w_{11},...,w_{1i},...,w_{1N})，w_{1i}=\frac{1}{N},i=1,2,...,N$$
**说明**：假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$.
**步骤2**.AdaBoost反复学习基本分类器，在每一轮$m=1,2,...,M$顺次地执行下列操作：
（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$：
$$G_m(x):X\rightarrow \{-1,+1\}$$
（b）计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率：
$$e_m=\sum_{i=1}^{N}P(G_m(x_i)\neq y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)\neq y_i)$$
**说明**：这里，`$w_{mi}$`表示第$m$轮中第`$i$`个实例的权值，`$\sum_{i=1}^{N}w_{mi}=1$`。这表明，**$G_m(x)$在加权的训练数据集上的分类误差率是被$G_m(x)$误分类样本的权值之和**，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系。
（c）计算基本分类器$G_m(x)$的系数$\alpha_m$（**分类误差率越小，权值越大**）:
$$\alpha_m=\frac{1}{2}log(\frac{1-e_m}{e_m})$$
这里的对数是自然对数。
**说明**：$\alpha_m$表示$G_m(x)$在最终分类器中的重要性。由上式可知，当$e_m\leq \frac{1}{2}$时，$\alpha_m\geq 0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。
（d）更新训练数据集的权值分布：
$$D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})$$
$$w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N$$
可简化写成：
$$w_{m+1,i}=\left\{\begin{matrix}   \frac{w_{mi}}{Z_m}e^{-\alpha_m} ,&        G_m\left(x_i\right)=y_i\\   \frac{w_{mi}}{Z_m}e^{\alpha_m} ,&        G_m\left(x_i\right)\ne y_i\\ \end{matrix}\right.$$
这里，$Z_m$是规范化因子
$$Z_m=\sum_{i=1}^{N}w_{mi}exp(-\alpha_my_iG_m(x_i))$$
它使$D_{m+1}$成为一个概率分布。
**说明**：被基本分类器$G_m(x)$误分类样本的权值得以扩大($\alpha_m\geq 0$,$e^{\alpha_m}\geq1$,乘以一个大于等于1的数)，而被正确分类样本的权值却得以缩小($\alpha_m\geq 0$,$0 \leq e^{-\alpha_m}\leq1$,乘以一个大于0小于1的数)。因此，误分类样本在下一轮学习中起更大的作用。**不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点**。
**步骤3**.构建基本分类器的线性组合：
$$f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)$$
得到最终分类器：
$$G(x)=sign(f(x))=sign(\sum_{m=1}^{M}\alpha_mG_m(x))$$
**说明**：线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(x)$的重要性，这里，所有$\alpha_m$之和不为1。$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的确信度。**利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点**。
注1：AdaBoost模型可以看成模型为加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法。
注2：可以看出标准的AdaBoost模型只支持二分类问题。
注3：做统计学习方法的例题和[Mit的例题](https://www.youtube.com/watch?v=gmok1h8wG-Q)，可以更好的理解。

## 四、参考资料
- 李航，统计学习方法
- 周志华，机器学习
- [MIT, Boosting(Adaboost)](https://www.youtube.com/watch?v=gmok1h8wG-Q)









