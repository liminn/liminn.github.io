---
title: 支持向量机
mathjax: true
top: true
date: 2017-12-12 17:40:50
categories: 
- 机器学习
tags:
- 线性可分支持向量机
- 线性支持向量机
- 非线性支持向量机
- 核函数
- smo算法
---
　　支持向量机(support vector machine, SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。线性支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题。
　　支持向量机学习方法包含构建由简至繁的模型：**线性可分支持向量机**、**线性支持向量机**、**非线性支持向量机**：

 - **线性可分支持向量机**：适用于训练数据线性可分，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为**硬间隔支持向量机**；
 - **线性支持向量机**：适用于训练数据近似线性可分，也就是存在一些特异点，将这些特异点去除后的样本线性可分（现实中的数据经常是线性不可分的）。通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为**软间隔支持向量机**；
 - **非线性支持向量机**：适用于训练数据线性不可分，通过核技巧即软间隔最大化，学习非线性支持向量机。
<!-- more --> 

**注**：
**核函数**：核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。
**核技巧**：通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。这样的方法成为核技巧。
　　
# 一、基本概念
（1）SVM的**分离超平面**：
$$w^*\cdot x+b^*=0\tag{1}$$
（2）SVM的**分类决策函数**为：
$$f(x)=sign(w^* \cdot x+b^*)\tag{2}$$
**注**：函数值只有正负1，二分类。
（3）**函数间隔**：
　　定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为：
$$\hat{\gamma_i}=y_i(w \cdot x_i+b)\tag{3}$$
　　定义超平面$(w,b)$关于训练集$T$的函数间隔为超平面$(w,b)$关于所有样本点$(x_i,y_i)$的函数间隔之最小值，即：
$$\hat{\gamma}=\min_{i=1,...,m}\hat{\gamma_i}\tag{4}$$
**注1**：函数间隔大于0，表示$w \cdot x_i+b$与$y_i$同号，分类正确；$|w \cdot x_i+b|$值越大，离分离超平面越远，分类预测的确信程度越高。所以用$y_i(w \cdot x_i+b)$来表示分类的正确性和确信度，这就是函数间隔。
**注2**：函数间隔可以表示分类预测的正确性及确信程度。但是选择分离超平面时，只有函数间隔还不够。因为只要成比例的改变$w$和$b$，例如将它们改变为$2w$和$2b$，超平面并没有改变，但函数间隔却成为原来的2倍。故需对超平面的法向量$w$加某些约束，如规范化$||w||=1$，使得间隔是确定的。此时，函数间隔即几何间隔。
（4）**几何间隔**：
　　定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：
$$\gamma_i=y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})\tag{5}$$
　　定义超平面$(w,b)$关于训练集$T$的几何间隔为超平面$(w,b)$关于所有样本点$(x_i,y_i)$的几何间隔之最小值，即：
$$\gamma=\min_{i=1,...,m}\gamma_i\tag{6}$$
**注**：超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔即**实例点到分离超平面的带符号的距离**，由[点到直线的距离公式](https://baike.baidu.com/item/%E7%82%B9%E5%88%B0%E7%9B%B4%E7%BA%BF%E8%B7%9D%E7%A6%BB/8673346)可推出。
　　从函数间隔和几何间隔的定义（式(3)~(6)）可知，函数间隔和几何间隔有如下关系：
$$\begin{align*}
\gamma_i=\frac{\hat{\gamma_i}}{||w||}\tag{7}\\
\gamma=\frac{\hat{\gamma}}{||w||}\tag{8}
\end{align*} $$
**注**：如果$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例地改变，超平面没有改变，函数间隔按此比例改变，而几何间隔不变。

# 二、线性可分支持向量机
　　支持向量机学习的基本思想是求解能够**正确划分训练数据集**并且**几何间隔最大**的分离超平面。
　　对**线性可分**的训练数据集，线性可分分离超平面有无穷多个，但几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为**硬间隔最大化**。
　　此时，学习到的线性分类器，称为**线性可分支持向量机**或**硬间隔支持向量机**。
注：几何间隔最大化的直观解释：对训练数据集找到的几何间隔最大化的超平面意味着以充分大的确信程度对训练数据进行分类。也就是说，不仅将正负实例分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。
## 2.1 原始最优化问题
　　（1）求最大间隔分离超平面的问题，可以表示为下面的约束最优化问题：
$$\begin{align*}
\max_{w,b}\; &\gamma \tag{9}\\
s.t.\; & y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})\geq\gamma,\;  i=1,2,...,m\tag{10}
\end{align*} $$
注：即我们希望最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma$，约束条件表示的是超平面$(w,b)$关于每个训练样本点的几何间隔至少是$\gamma$。
　　（2）根据函数间隔与几何间隔的关系式(8)，约束问题可以等价于：
$$\begin{align*}
\max_{w,b}\; &\frac{\hat{\gamma}}{||w||}\tag{11}\\
s.t.\; & y_i(w\cdot x_i+b)\geq\hat{\gamma},\; i=1,2,...,m\tag{12}
\end{align*} $$
　　（3）由于式(12)中函数间隔$\hat{\gamma}$的取值不影响优化问题的解（见注解），因此可以取$\hat{\gamma}=1$。将$\hat{\gamma}=1$代入式(11~12)的最优化问题，且最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，于是就得到下面的线性可分支持向量机学习的最优化问题：
$$\begin{align*}
\min_{w,b}\; &\frac{1}{2}||w||^2 \tag{13}\\
s.t.\; & y_i(w\cdot x_i+b)-1\geq0,\, i=1,2,...,m\tag{14}
\end{align*}$$
　　这是一个凸优化问题中的凸二次规划问题。
　　如果求出了约束条件最优化问题式(13)~(14)的解`$w^*$`，`$b^*$`，那么就可以得到最大间隔分离超平面`$w^*\cdot x+b^*=0$`及分类决策函数`$f(x)=sign(w^* \cdot x+b^*)$`，即线性可分支持向量机模型。
**注**：式(12)中函数间隔$\hat{\gamma}$的取值不影响优化问题的解。假设将$w$和$b$按比例改变为$\lambda w$和$\lambda b$，这时函数间隔成为$\lambda \hat{\gamma}$。函数间隔的这一改变既对式(12)的不等数约束没有影响，又对式(11)的目标函数的优化没有影响，也就是说，它产生一个等价的最优化问题。
## 2.2 支持向量和间隔边界
　　在线性可分的情况下，**训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量**。支持向量是使约束条件式(14)等号成立的点，即
$$y_i(w\cdot x_i+ b)-1=0\tag{15}$$
　　对$y_i=+1$的正例点，支持向量在超平面
$$H_1:w \cdot x+b=1\tag{16}$$
　　对$y_i=-1$的负例点，支持向量在超平面
$$H_2:w \cdot x+b=-1\tag{17}$$
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%92%8C%E9%97%B4%E9%9A%94%E8%BE%B9%E7%95%8C.JPG" width="50%" height="50%">
　　　　　　　　　　　　　　　　　图1，支持向量和间隔边界
　　如图1，在$H_1$和$H_2$上的点就是**支持向量**，$H_1$和$H_2$称为**间隔边界**，$H_1$与$H_2$之间的距离称为**间隔**(margin)。
注1：$H_1$与$H_2$平行，并且没有实例点落在它们中间。在$H_1$与$H_2$之间形成一条长带，**分离超平面与它们平行且位于它们中央**。长带的宽度，即$H_1$与$H_2$之间的距离称为间隔。因为$H_1$和$H_2$的函数间隔为1，故各自与分离超平面的几何间隔为$\frac{1}{||w||}$，即**间隔为$\frac{2}{||w||}$。**
注2：在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。**由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机**。支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。
## 2.3 对偶最优化问题
　　为了求解线性可分支持向量机的最优化问题（式(13)~(14)），将它作为**原始最优化问题**，应用拉格朗日对偶性，通过求解**对偶问题**(dual problem)得到**原始问题**(primal problem)的最优解，这就是线性可分支持向量机的**对偶算法**。这样做的优点，**一是对偶问题往往更容易求解，二是自然引入核函数，进而推广到非线性分类问题**。
　　（1）构建拉格朗日函数
　　对每一个不等式约束，引进拉格朗日乘子$\alpha_i\geq0,\ i=1,2,..,m$，定义拉格朗日函数：
$$L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^{m}\alpha_i(1-y_i(w\cdot x_i+b))\tag{18}$$
　　式中，$\alpha=(\alpha_1,\alpha_2,...,\alpha_m)^T$为拉格朗日乘子向量。
　　（2）对偶问题
　　根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
$$\max_\alpha\, \min_{w,b}L(w,b,\alpha)$$
　　所以，为了得到对偶问题的解，需要先求$L(w,b,\alpha)$对$w,b$的极小，再求对$\alpha$的极大。
　　（3）对偶问题——求$\min\limits_{w,b}L(w,b,\alpha)$
　　将拉格朗日函数$L(w,b,\alpha)$分别对$w,b$求偏导数并令其等于0，
$$\nabla_wL(w,b,\alpha)=w-\sum_{i=1}^{m}\alpha_i y_i x_i=0\\
\nabla_bL(w,b,\alpha)=-\sum_{i=1}^{m}\alpha_i y_i=0$$
　　得，
$$\begin{align*}
w=\sum_{i=1}^{m}\alpha_i y_i x_i\tag{19}\\
\sum_{i=1}^{m}\alpha_i y_i=0\tag{20}
\end{align*} $$
　　将式(19)代入拉格朗日函数（式(18)），并利用式(20),即得
$$\min_{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{m}\alpha_i$$
　　（4）对偶问题——求$\min_{w,b}L(w,b,\alpha)$对$\alpha$的极大，即是对偶问题
$$\begin{align*}
\max_{\alpha}\; & -\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{m}\alpha_i\tag{21}\\
s.t.\; &\sum_{i=1}^m\alpha_iy_i=0\\
& \alpha_i\geq0,\; i=1,2,...m
\end{align*} $$
　　（5）将上式(21)的目标函数由求极大转换成为求极小，就得到下面与之等价的**对偶最优化问题**：
$$\begin{align*}
\min_{\alpha}\; & \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{m}\alpha_i\tag{22}\\
s.t.\; &\sum_{i=1}^m\alpha_iy_i=0\tag{23}\\
& \alpha_i\geq0,\; i=1,2,...m\tag{24}
\end{align*}$$
　　考虑原始最优问题(式(13)~(14))和对偶最优化问题(式(22)~(24))，原始问题满足**定理C.2(见书中，此处略)**的条件，故存在$w^*$,$\alpha^*$，$w^*$是原问题的解，$\alpha^*$是对偶问题的解。**这意味着求解原始问题(式(13)~(14))可以转换为求解对偶问题(式(22)~(24))**。
　　依据定理C.3(见书中，此处略)，**`$w^*$`和`$\alpha^*$`分别是原始问题和对偶问题的解的充要条件是`$w^*$`，`$\alpha^*$`满足KKT条件**。
## 2.4 KKT条件
原问题可行：　　
$y_i(w^*\cdot x_i+b^*)-1\geq0,\; i=1,2,...,m \tag{25}$
互补松弛条件(complementary slackness):
$\alpha_i^*(y_i(w^*\cdot x_i+b^*)-1)=0,\; i=1,2,...,m \tag{26}$
对偶可行：
$\alpha_i^*\geq0,\; i=1,2,...,m \tag{27}$
对偶内在优化：
$\nabla_wL(w^*,b^*,\alpha^*)=w^*-\sum_{i=1}^{m}\alpha_i^*y_ix_i=0 \tag{28}$
$\nabla_bL(w^*,b^*,\alpha^*)=-\sum_{i=1}^{m}\alpha_i^*y_i=0 \tag{29}$

## 2.5 对偶问题最优解与原始问题最优解的对应关系
　　设$\alpha=(\alpha_1,\alpha_2,...,\alpha_m)^T$是对偶最优化问题(式(22)~(24))的解，则存在下标$j$，使得$\alpha_j>0$，并可按下式求得原始最优化问题(式(13)~(14))的解$w^*$,$b^*$：
$$\begin{align*}
w^*=\sum_{i=1}^{m}\alpha_i^*y_ix_i \tag{30}\\
b^*=y_j-\sum_{i=1}^{m}\alpha_i^*y_i(x_i\cdot x_j) \tag{31}
\end{align*}$$
**注1**：$w^*$从KKT条件中式(28)中直接得出。
**注2**：$b^*$的得出：从KKT条件知道，至少有一个$\alpha_j^*>0$（反证法，假设$\alpha^*=0$，由式(28)得$w^*=0$，而$w^*=0$不是原始最优化问题(式(13)~(14))的解，矛盾）。结合KKT中互补松弛条件(式(26))，对此$j$有：
$y_j(w^* \cdot x_j + b^*)-1=0 \tag{32}$
将$w^*$的解(式(30))代入式(32)并注意到$y_j^2=1$，即得
$b^*=y_j-\sum_{i=1}^{m}\alpha_i^*y_i(x_i\cdot x_j) \tag{31}$
　　
　　求得解$w^*$，$b^*$后，**分离超平面**可写成：
$$\begin{align*}
w^*\cdot x+b^*=0 \tag{33}\\
\sum_{i=1}^{m}\alpha_i^*y_i (x \cdot x_i)+b^*=0 \tag{34}
\end{align*}$$
　　**分类决策函数**可写成：
$$\begin{align*}
f(x)=sign(w^* \cdot x+b^*) \tag{35}\\
f(x)=sign(\sum_{i=1}^{m}\alpha_i^*y_i (x \cdot x_i)+b^*) \tag{36}
\end{align*}$$
　　这就是说，分类决策函数只依赖于输入$x$和训练样本输入的内积。**式(36)称为线性可分支持向量机的对偶形式**。
　　综上，对于给定的线性可分训练数据集，可以首先求对偶问题（式(22)~(24)）的解$\alpha^*$；再利用式(30)和式(31)求得原始问题的解$w^*$和$b^*$；从而得到分离超平面及分类决策函数。这种算法称为**线性可分支持向量机的对偶学习算法**，是线性可分支持向量机的基本算法。

## 2.6 支持向量
在线性可分支持向量机中，由式(30)和式(31)可知，**`$w^*$`和`$b^*$`只依赖于训练数据中对应于`$\alpha_i^*>0$`的样本点`$(x_i,y_i)$`**，而其他样本点对`$w^*$`和`$b^*$`没有影响。**将训练数据中对应于`$\alpha_i^*>0$`的实例点`$x_i \in R^n$`称为支持向量**。
支持向量一定在间隔边界上。由KKT互补松弛条件(式(26))可知，对应于于`$\alpha_i^*>0$`的实例`$x_i$`，有
$$y_i(w^*\cdot x_i+b^*)-1=0$$
或
$$w^*\cdot x_i+b^*=\pm1$$
即$x_i$一定在间隔边界上。这里的支持向量的定义与前面给出的支持向量的定义是一致的。

# 三、线性支持向量机

## 2.1 原始最优化问题
**线性不可分的线性支持向量机的原始问题**如下：
$$\begin{align*}
\min_{w,b,\xi}\; & \frac{1}{2}||w||^2+C\sum_{i=1}^{m}\xi_i \tag{37}\\
s.t.\; & y_i(w\cdot x_i+b)\geq1-\xi_i, \; i=1,2,...,m\tag{38}\\
& \xi_i\geq0,\; i=1,2,...,m\tag{39}
\end{align*}$$
**注1**：线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件（式(14)）。为了解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\xi_i\geq0$，使函数间隔加上松弛变量大于等于1。这样约束条件变为式(38)。
**注2**：同时，对每一个松弛变量$\xi_i$，支付一个代价$\xi_i$。目标函数由原来的$\frac{1}{2}||w||^2$变成式(37)。这里，$C>0$称为惩罚参数，一般由应用问题决定，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数式(37)包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，$C$是调和二者的系数。
　　有了上面的思路，可以和训练数据集线性可分时一样来考虑训练数据集线性不可分时的线性支持向量机学习问题。相应于硬间隔最大化，它称为**软间隔最大化**。
　　原问题式(37)~式(39)是一个凸二次规划问题，因而关于$(w,b,\xi)$的解是存在的。
　　设原问题式(37)~式(39)的解是`$w^*$`，`$b^*$`，于是可以得到分离超平面`$w^* \cdot x + b^* =0$`及分类决策函数`$f(x)=sign(w^* \cdot x + b^*)$`。成这样的模型为训练样本线性不可分时的线性支持向量机，简称**线性支持向量机**。
## 2.2 对偶最优化问题
（1）构建拉格朗日函数
　　原始最优化问题（式(37)~(39)）的拉格朗日函数是
$$
L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^{m}\xi_i-\sum_{i=1}^{m}\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum_{i=1}^{m}\mu_i\xi_i
\tag{40}
$$
　　其中，$\alpha_i\geq0,\mu_i\geq0$.
（2）对偶问题
　　对偶问题是拉格朗日函数的极大极小问题：
$$\max_\alpha\; \min_{w,b,\xi}\,L(w,b,\xi,\alpha,\mu)$$
　　所以，为了得到对偶问题的解，需要先求$L(w,b,\xi,\alpha,\mu)$对$w,b,\xi$的极小，再求对$\alpha$的极大。
（3）对偶问题——求$\min\limits_{w,b,\xi}\,L(w,b,\xi,\alpha,\mu)$
　　通过
$$\begin{align*}
\nabla_wL(w,b,\xi,\alpha,\mu)=w-\sum_{i=1}^{m}\alpha_i y_ix_i=0\\
\nabla_bL(w,b,\xi,\alpha,\mu)=-\sum_{i=1}^{m}\alpha_i y_i=0\\
\nabla_{\xi}L(w,b,\xi,\alpha,\mu)=C-\alpha_i-\mu_i=0
\end{align*}$$
　　得
$$\begin{align*}
w=\sum_{i=1}^{m}\alpha_iy_ix_i\tag{41}\\
\sum_{i=1}^{m}\alpha_iy_i=0\tag{42}\\
C-\alpha_i-\mu_i=0\tag{43}
\end{align*}$$
　　将式(41)到式(43)代入式(40)，得
$$\min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)=-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{m}\alpha_i$$
（4）对偶问题——求$\min\limits_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$对$\alpha$的极大，即得对偶问题
$$\begin{align*}
\max_{\alpha}\; & -\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{m}\alpha_i\tag{44}\\
s.t.\; & \sum_{i=1}^m\alpha_iy_i=0\tag{45}\\
& C-\alpha_i-\mu_i=0\tag{46}\\
& \alpha_i\geq0,\; i=1,2,...m\tag{47}\\
& \mu_i\geq0,\; i=1,2,...,m\tag{48}
\end{align*} 
$$
　　将对偶最优化问题式(44)~式(48)进行变换：利用等式约束式(46)消去$\mu_i$，从而只留下变量$\alpha_i$，并将约束式(46)~式(48)写成
$$0\leq\alpha_i\leq C\tag{49}$$
（5）再将对目标函数求极大极小转为求极小，于是得到**对偶问题**(50)~(52).
$$\begin{align*}
\min_{\alpha}\; & \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{m}\alpha_i\tag{50}\\
s.t.\; & \sum_{i=1}^m\alpha_iy_i=0\tag{51}\\
& 0\leq \alpha_i\leq C,\;i=1,2,...m\tag{52}\\
\end{align*}$$
　　综上，原始最优化问题(式(37)～式(39))的对偶最优化问题为式(50)~式(51)。可以通过求解对偶问题而得到原始问题的解，进而确定分离超平面和决策函数。
　　有定理可证，**求解原始问题(式(37)～式(39))可以转换为求解对偶问题式(50)~式(52)**。
　　有定理可证，**`$w^*$`和`$\alpha^*$`分别是原始问题和对偶问题的解的充要条件是`$w^*$`，`$\alpha^*$`满足KKT条件**。
## 2.3 KKT条件
原问题可行：
$$\begin{align*}
y_i(w^*\cdot x_i+ & b^*)-1+\xi_i^*\geq0 \tag{53} \\
& \xi_i^*\geq0 \tag{54}
\end{align*}$$
互补松弛条件(complementary slackness):
$$\begin{align*}
\alpha_i^*(y_i(w^*\cdot x_i & + b^*)-1 +\xi_i^*)=0 \tag{55}\\
& \mu_i^*\xi_i^*=0\tag{56}
\end{align*}$$
对偶问题可行：
$$\begin{align*}
\alpha_i^*\geq0 \tag{57}\\
\mu_i^*\geq0 \tag{58}
\end{align*}$$
对偶内在优化：
$$\begin{align*}
\nabla_wL(w^*,b^*,\xi^*,\alpha^*,\mu^*)=w^*-\sum_{i=1}^{m}\alpha_i^*y_ix_i=0 \tag{59}\\
\nabla_bL(w^*,b^*,\xi^*,\alpha^*,\mu^*)=-\sum_{i=1}^{m}\alpha_i^*y_i=0 \tag{60}\\
\nabla_{\xi}L(w^*,b^*,\xi^*,\alpha^*,\mu^*)=C-\alpha_i^*-\mu_i^*=0 \tag{61}
\end{align*}$$

## 2.4 对偶问题最优解与原始问题最优解的对应关系
　　设`$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_m^*)$`是对偶问题（式(50)~式(52)）的一个解，若存在`$\alpha^*$`的一个分量`$\alpha_j^*$`，`$0<\alpha_j^* < C$`，则原始问题式（(37)～式(39)）的解`$w^*,b^*$`可按下式求得：
$$\begin{align*}
w^*=\sum_{i=1}^{m}\alpha_i^*y_ix_i\tag{62}\\
b^*=y_j-\sum_{i=1}^{m}y_i\alpha_i^*(x_i\cdot x_j)\tag{63}
\end{align*}$$
**注1**：`$w^*$`从KKT条件中式(59)中直接得出。
**注2**：`$b^*$`的得出：从KKT条件知道，至少有一个`$0<\alpha_j^* < C$`（反证法，假设`$\alpha^*=0$`，由式(59)得`$w^*=0$`，而`$w^*=0$`不是原始最优化问题(式(37)～式(39)的解，矛盾）。结合KKT中互补松弛条件(式(55)~式(56))，对此$j$有：
`$y_j(w^* \cdot x_j + b^*)-1=0 \tag{64}$`
将`$w^*$`的解(式(62))代入式(64)并注意到`$y_j^2=1$`，即得式(63)。（结合式(61)和(56)，此时的$$\xi_j^*=0$$）
　　求得解`$w^*$`，`$b^*$`后，**分离超平面**可写成：
$$\begin{align*}
w^*\cdot x+b^*=0 \tag{65}\\
\sum_{i=1}^{m}\alpha_i^*y_i (x \cdot x_i)+b^*=0 \tag{66}
\end{align*}$$
　　**分类决策函数**可写成：
$$\begin{align*}
f(x)=sign(w^* \cdot x+b^*) \tag{67}\\
f(x)=sign(\sum_{i=1}^{m}\alpha_i^*y_i (x \cdot x_i)+b^*) \tag{68}
\end{align*}$$
　　这就是说，分类决策函数只依赖于输入$x$和训练样本输入的内积。**式(68)称为线性支持向量机的对偶形式**。
## 2.5 支持向量
　　**软间隔的支持向量**：在线性不可分的情况下，将对偶问题（式(50)~式(52)）的解`$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_m^*)$`中**对应于`$\alpha_i^*>0$`的样本点`$(x_i,y_i)$`的实例`$x_i$`称为支持向量。**
　　结合互补松弛条件（式(55)~式(56)）及式(61)，得
$$
\alpha_i^*(y_i(w^*\cdot x_i+b^*)-1+\xi_i^*)=0, i=1,2,...,m\\
(C-\alpha_i^*)\xi_i^*=0
$$
　　软间隔的支持向量$x_i$**或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。**

- `$\alpha_i^*=0$`时：非支持向量，`$\xi_i=0$`，没犯错，远离间隔边界或在间隔边界上；
- `$0<\alpha_i^*<C$`时：支持向量，`$\xi_i=0$`，支持向量`$x_i$`在在间隔边界上，决定了`$b^*$`；
- `$\alpha_i^*=C$`时：支持向量，`$\xi_i \neq 0$`,`$\xi_i>0$`，`$\xi_i$`记录了违反间隔边界的数量或大小
 - `$0<\xi_i < 1$`：分类正确,`$x_i$`落在间隔边界与分离超平面之间；
 - `$\xi_i = 1 $`：`$w^*\cdot x_i+b^*=0$`,`$x_i$`落在分离超平面上；
 - `$\xi_i >1 $`：样本点分类错误，`$x_i$`落在分离超平面误分一侧。

# 四、非线性支持向量机
　　对解线性分类问题，线性分类支持向量机是一种非常有效的方法。但是，通常分类问题是非线性的，这时可以使用非线性支持向量机，其主要特点是利用核技巧。
　　用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。核技巧就属于这样的方法。
## 4.1 核函数的定义
　　假设$X$是输入空间，$H$是特征空间，如果存在一个从$X$到$H$的映射函数：
$$\phi(x):X\rightarrow H$$
　　使得所有$x,z\in X$,函数$K(x,z)$满足条件：
$$K(x,z)=\phi(x)\cdot \phi(z)$$
　　则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot \phi(z)$为$\phi(x)$和$\phi(z)$的内积。
　　**核技巧的想法是：在学习与预测中只定义核函数$K(x,z)$,而不是显示地定义映射函数$\phi$。通常，直接计算$K(x,z)$比较容易，而通过$\phi(x)$和$\phi(z)$计算$K(x,z)$并不容易。**注意，$\phi$是输入空间$R^n$到特征空间$H$的映射，特征空间$H$一般是高维的，甚至是无穷维的。
## 4.2 核技巧在支持向量机中的应用
　　我们注意到在线性支持向量机的对偶问题中，无论是在目标函数还是在决策函数(分离超平面)都只涉及输入实例与实例之间的内积。
　　在对偶问题的目标函数(式(50))中的内积`$x_i\cdot x_j$`可以用核函数`$K(x_i,x_j)=\phi(x_i)\cdot \phi(x_j)$`来代替。此时对偶问题的目标函数变为：
　　$$W(\alpha)=\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i\cdot x_j)-\sum_{i=1}^{m}\alpha_i \tag{69}$$
　　同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数变为：
　　$$f(x)=sign(\sum_{i=1}^{m}\alpha_i^*y_i \phi(x) \cdot \phi(x_i)+b^*) \\
　　=sign(\sum_{i=1}^{m}\alpha_i^*y_i K(x \cdot x_i)+b^*)   \tag{70}$$
　　这等价于经过映射函数$\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i \cdot x_j$变换为特征空间中的内积$\phi(x_i) \cdot \phi(x_j)$，在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性时，学习到的含有核函数的支持向量机是非线性分类模型。
　　也就是说，在核函数`$K(x,z)$`给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是在隐式地特征空间进行的，不需要显示地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。
##  4.3 常用核函数
　　（1）多项式核函数：
$$K(x,z)=(\gamma \cdot x\cdot z+ \zeta)^p  \tag{71}$$
　　式中，$\gamma\in R$且$\gamma>0$,$\zeta\in R$且$\zeta\geq0$
　　特殊情况：线性核：$K_1(x,z)=(1 \cdot x\cdot z+0)^1$
　　（2）高斯核函数：
$$K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2}) \tag{72}$$
　　或写成$K(x,z)=exp(-\gamma||x-z||^2)$,$\gamma>0$
　　**注**：正态分布的PDF为：$f(x;\mu,\sigma)= \frac{1}{\sigma \sqrt{2 \pi}}exp(-\frac{(x-\mu)^2}{2{\sigma}^2})$
## 4.４ 非线性支持向量机学习算法
　　如上所述，利用核技巧，可以将线性分类的学习方法应用到非线性分类问题中去。将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。
　　（1）选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题
$$\begin{align*}
\min_{\alpha}\; & \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{m}\alpha_i\tag{73}\\
s.t.\; & \sum_{i=1}^m\alpha_iy_i=0\tag{74}\\
& 0\leq \alpha_i\leq C,\; i=1,2,...m\tag{75}\\
\end{align*} 
$$
　　求得最优解`$\alpha^*=(\alpha^*_1,\alpha^*_2,...,\alpha^*_m)$`.
　　(2)选择`$\alpha^*$`的一个正分量`$0<\alpha^*_j < C$`，计算
$$b^*=y_j-\sum_{i=1}^{m}\alpha_i^*y_iK(x_i \cdot x_j)$$
　　(3)构造决策函数：
$$f(x)=sign(w^*\cdot x+b^*)=sign(\sum_{i=1}^{m}\alpha^*_iy_iK(x\cdot x_i)+b^*)$$

# 五、SMO算法（待补）
SMO算法的思想： 
（1）SMO是一种启发式算法，选择两个变量固定其他变量，针对选择的两个变量构造二次规划问题，二次规划问题可以直接得到解析解；SMO算法将原问题 不断地分解为子问题并对子问题进行求解进而达到求解原问题的目的； 
（2）之所以选择每次更新两个变量是因为目标函数中的第二个约束，若固定其他的变量，那么最后一个变量也随之确定，因此需要更新两个变量。 
# 六、SVM优缺点（待补）
1.优点
（1）可用于线性/非线性分类，也可以用于回归；
（2）低泛化误差；
（3）容易解释；
（4）计算复杂度较低；
（5）处理小样本，非线性，高维数问题；
2.缺点
（1）对参数和核函数的选择比较敏感；
（2）原始的SVM只比较擅长处理二分类问题；
# 七、参考资料
- 李航，统计学习方法
- [林轩田，机器学习技法](https://www.youtube.com/watch?v=A-GxGCCAIrg&list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2)
- [Andrew Ng, Machine Learning](https://www.coursera.org/learn/machine-learning)
- 周志华，机器学习


