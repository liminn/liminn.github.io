---
title: kNN算法
mathjax: true
date: 2017-12-5 20:52:50
categories: 
- 机器学习
---
k近邻法（k-nearest neighbor, kNN）是一种基本分类与回归方法。k近邻不具有显式的学习过程，k近邻实际上是利用训练数据集对特征空间进行划分，并作为其分类的“模型”。k近邻法的三个基本要素：$k$值的选择、距离度量及分类决策规则。
本文记录分类问题中的kNN算法。
<!-- more --> 
# 一、kNN算法
kNN算法简单且直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的$k$个实例，这$k$个实例中的多数属于某个类，就把该输入实例分为这个类。
## 1.1 kNN算法
输入：训练数据集$T= \lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \rbrace$；实例特征向量$x$；
输出：实例$x$所属的类$y$
（1）根据所给的距离度量，在训练集$T$中找到与$x$最近的$k$个点，涵盖这$k$个点的$x$的邻域记作$N_k(x)$；
（2）在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$：
$$y = arg\max_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j),\ \ i=1, 2,…, N;\ \ j=1, 2,…, K$$
其中$I$为指示函数，即当$y_i = c_j$时$I$为1，否则为0。
## 1.2 kNN的三大基本要素
### 距离度量
特征空间中两个实例点的距离是两个实例点相似程度的反映。
一般最常用的距离函数是欧氏距离，也称作$L_2$距离。也可使用其他距离，如更一般的$L_p$距离。
设特征空间$X$是$n$维实数向量空间$R^n$，$x_i,x_j \in R^n$，$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$，$x_j=(x_j^{(1)},x_j^{(2)},…, x_j^{(n)})^T$，则$x_i, x_j$的$L_p$距离定义为
$$L_p(x_i,x_j)=(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$$
这里$p \geq 1$。当$p=2$时称为欧氏距离，$p=1$时称为曼哈顿距离。
不同的距离度量所确定的最近邻点是不同的。
注：需对特征向量各维度进行归一化，否则数值较大的特征将占主导因素。
### $k$值的选择
$k$值较小意味着整体模型变得复杂，容易发生过拟合。
$k$值较大意味着整体模型变得简单，容易发生欠拟合。
在应用中，$k$值一般取一个比较小的数值。通常采用交叉验证法来选取最优的$k$值。
### 分类决策规则
kNN中的分类决策规则通常是多数表决，即由输入实例的$k$个近邻的训练实例中的多数类决定输入实例的类。
多数表决规则等价于经验风险最小化。
# 二、kd树
## 2.1 kd树简介
实现kNN算法的核心问题是如何对训练数据进行快速k近邻搜索，尤其是在特征空间的维数大及训练数据容量大的情况下。
kNN算法最简单的实现方法是线性扫描，即计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，故此方法不可行。
常用的有效方法是使用特殊的结构存储训练数据，以减少计算距离的次数，从而提高k近邻搜索的效率。如用kd树存储训练数据，然后搜索kd树。
kd树是一种对$k$维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。**kd树是二叉树**，表示对$k$维空间的一个划分（partition）。构造kd树相当于不断地用垂直于坐标轴的超平面将$k$维空间切分，构成一系列的$k$维超矩形区域。kd树的每个结点对应于一个$k$维超矩形区域。
注：kd树是存储$k$维空间数据的树结构，这里的$k$与k近邻法的$k$意义不同。
## 2.2 构造KD树
输入：$k$维空间数据集$T= \lbrace x_1, x_2,…, x_N \rbrace$，其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T$,$i=1,2,…,N$
输出：kd树
（1）开始：构造根结点，根结点对应于包含$T$的$k$维空间的超矩形区域。选择$x^{(1)}$为坐标轴，以$T$中所有实例的$x^{(1)}$坐标的**中位数**为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
由根结点（深度为0）生成深度为1的左、右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应于坐标$x^{(1)}$大于切分点的子区域。
将落在切分超平面上的实例点保存在根结点。
注：中位数：一组数据按大小顺序排列起来，处在中间位置的一个树或者最中间两个数的平均值。
（2）重复：**对于深度为$j$的结点，选择$x^{(l)}$为切分的坐标轴，$l=(j+1)\ mod\ k$**，以该结点区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。
由该结点生成深度为$j+1$的左、右结点：左子结点对应坐标$x^{(l)}$小于切分点的子区域；右子结点对应坐标$x^{(l)}$大于切分点的子区域。
将落在切分超平面上的实例点保存在该结点。
（3）直到两个区域**没有实例存在时停止**。从而形成kd树的区域划分。

示例见《统计学习方法书》中或[kd树算法之详细篇（推荐）](https://www.joinquant.com/post/2843)中。
## 2.3 搜索KD树
输入：已构造的kd树；目标点p；
输出：p的k近邻
（零）设$L$为一个有$k$个空位的列表，用于保存已搜寻到的最近点。
（一）根据$p$的坐标值和每个节点的切分向下搜索（也就是说，如果树的节点是按照$x_r=a$进行切分，并且$p$的$r$坐标小于$a$，则向左枝进行搜索；反之则向右枝进行搜索）。
（二）当达到一个底部节点时，将其标记为访问过。如果$L$里不足$k$个点，则将当前节点的特征坐标加入$L$；如果$L$满并且当前节点的特征与$p$的距离小于$L$里最长的距离，则用当前特征替换掉$L$中离$p$最远的点。
（三）如果当前节点不是整棵树最顶端节点，执行（a）；反之，输出$L$，算法完成。
　　（a）向上爬一个节点。如果当前（向上爬之后的）节点未曾被访问过，将其标记为被访问过，然后执行（1）和（2）；如果当前节点被访问过，再次执行（a）。
　　　　（1）如果此时$L$里不足$k$个点，则将节点特征加入$L$；如果$L$中已满$k$个点，且当前节点与$p$的距离小于$L$里最长的距离，则用节点特征替换掉$L$中离最远的点。
　　　　（2）计算$p$和当前节点切分线的距离。如果该距离大于等于$L$中距离$p$最远的距离并且$L$中已有$k$个点，则在切分线另一边不会有更近的点，执行(三)；如果该距离小于$L$中最远的距离或者$L$中不足$k$个点，则切分线另一边可能有更近的点，因此在当前节点的另一个枝从(一)开始执行。

示例见《统计学习方法书》中或[kd树算法之详细篇（推荐）](https://www.joinquant.com/post/2843)中。
# 三、KNN模型的优缺点（待改进）
1.优点：
（1）思想简单，理论成熟，既可以用来做分类也可以用来做回归
（2）可用于非线性分类
（3）训练时间复杂度为$O(n)$
（4）准确度高，对数据没有假设，对异常值不敏感
2.缺点：
（1）计算量大
（2）样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）
（3）需要大量的内存
# 四、参考资料
- 李航，统计机器学习方法
- [肖睿，一只兔子帮你理解KNN](https://www.joinquant.com/post/2227?f=study&m=math)
- [肖睿，kd树算法之思路篇](https://www.joinquant.com/post/2627)
- [肖睿，kd树算法之详细篇](https://www.joinquant.com/post/2843)





