---
title: 随机森林
mathjax: true
date: 2017-12-15 20:20:50
categories: 
- 机器学习
tags:
---
# 1.Bagging
Bagging是并行式集成学习方法最著名的带代表。Bagging的基本流程：
- Bagging通过自主采样法(bootstrap sampling)在给定$m$个样本的数据集中，经过$m$次随机采样操作，得到含$m$个样本的采样集（有放回，初始训练集中有的样本在采样集里多次出现，有的则从未出现，初始训练集中约有63.2%的样本出现在采样集中）。
- 照这样，采样出$T$个含$m$个训练样本的采样集，然后基于每个采样集训练出一个基学习器。
- 再将这$T$个基学习器进行结合。在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。

此外：
- 可以证明：Bagging可以降低模型的方差。
- 与标准Adaboost只适用于二分类任务不同，Bagging能不经修改地用于多分类、回归等任务。

<!-- more --> 
# 2.随机森林
随机森林(Random Forest, RF)是Bagging的一个扩展变体。**RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择**。
**随机森林的建立**:基本就是两个步骤，随机采样与完全分裂。
（1）随机采样
　　首先是两个随机采样的过程，RF对输入的数据要进行行、列的采样。
对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为$m$个，那么采样的样本也为$m$个，这选择好了的$m$个样本用来训练一个决策树，作为决策树根节点处的样本，同时使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现过拟合。
- 注：只进行Bagging得到的多颗树，由于只是训练数据有一些不同，故得到的多颗树是高度相关的，因此带来的方差减少有限。故进一步采用随机选择一部分特征的方式，降低树的相关性。

　　对于列采样，从$d$个属性中，选择$k$个($k << d$)，即：当每个样本有$d$个属性时，在决策树的每个节点需要分裂时，随机从这$d$个属性中选取出$k$个属性，满足条件$k << d$。
（2）完全分裂
　　对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。分裂的办法是：采用上面说的列采样的过程从这$k$个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
　　决策树形成过程中每个节点都要按完全分裂的方式来分裂，一直到不能够再分裂为止（如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。
（3）其他
　　首先，以上是未剪枝决策树的生成过程，一般很多的决策树算法都会包含剪枝过程来避免过拟合。但是由于随机森林的两个随机采样的过程保证了随机性，所以就算不剪枝也不容易出现过拟合，这也是随机森林的优势之一。
　　此外，随机森林有2个参数需要人为控制，一个是森林中树的数量，一般建议取很大。另一个是$k$的大小，推荐$k$的值为$log_2d$。

# 3.参考资料
- 周志华，机器学习
- [Nando de Freitas, CPS540](https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6)
- [林轩田，机器学习技法](https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2)
- [holybin，机器学习中的算法：决策树模型组合之随机森林](http://blog.csdn.net/holybin/article/details/25653597)


















