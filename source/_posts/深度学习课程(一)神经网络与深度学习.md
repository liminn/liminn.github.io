---
title: 深度学习课程(一)神经网络与深度学习
mathjax: true
top: true
date: 2017-12-21 15:40:50
categories: 
- 深度学习
tags:
---
# **1.深度学习介绍**
## **用神经网络进行监督学习**
到目前为止，由神经网络创造的经济价值几乎都是基于一种机器学习方法——监督学习。
下面是目前主要的几种应用示例：
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.JPG" width="50%" height="50%">不同的情况，一般使用不同的网络模型：
- 对于房价预测和在线广告，使用标准的神经网络模型；
- 对于计算机视觉领域，使用CNN(convolution neural network)；
- 对于序列数据，如音频(如语音识别)和语言(如机器翻译)，使用RNN(recurrent neural network)。
- 对于更复杂的应用，如无人驾驶，使用混合的神经网络结构。

<!-- more --> 

神经网络类型示例:
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B1%BB%E5%9E%8B%E4%B8%BE%E4%BE%8B.JPG" width="60%" height="50%">结构化数据与非结构化数据：
数据类型一般分为两种：结构化数据和非结构化数据。
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE.JPG" width="70%" height="50%">

## **为什么深度学习会兴起？**
三个要素：数据量、计算能力、算法
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%B4%E8%B5%B7.jpg" width="60%" height="50%">

# **2.神经网络基础**
## **2.1 将逻辑回归作为一个神经网络**
### **深度学习符号标准**
该专项课程，规范了深度学习所用到的所有符号的表示方法：
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%AC%A6%E5%8F%B7%E6%A0%87%E5%87%861.JPG" width="50%" height="50%"> <img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%AC%A6%E5%8F%B7%E6%A0%87%E5%87%862.JPG" width="50%" height="50%">
### **二元分类**
二元分类(Binary Classification)就是输出`$y \in \{0,1\}$`。
以一个图像识别问题为例，判断图片中是否有猫存在，0代表不是猫，1代表是猫。
### **逻辑回归**
- Given $x$,want $\hat{y}=P(y=1|x)$，其中$x\in R^{n_x}$,$0\leq\hat{y}\leq1$
- 参数：$w\in R^{n_x}$,$b\in R$
- 输出：$\hat{y}=\sigma(w^Tx+b)$，其中$\sigma()$为sigmoid函数，$\sigma(z)=\frac{1}{1+e^{-z}}$。
- 注意：本课程中不采用`$x_0=1,x\in R^{n_x+1}$`，`$\hat{y}=\sigma(\theta^Tx)$`,`$\theta=[\theta_0,\theta_1,...,\theta_{n_x}]^T$`的做法。

### **逻辑回归代价函数**
- Given `$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$`, want $\hat{y}^{(i)}=y^{(i)}$
- Loss Function: $L(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))$
- Cost Function: $$J(w,b)= \frac{1}{m} \sum\limits_{i=1}^m L(\hat{y}^{(i)},y^{(i)})\\
=-\frac{1}{m} \sum\limits_{i=1}^m [y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]$$

### **梯度下降**
- want to find $w$,$b$ that minimize $J(w,b)$
- $ w:= w-\alpha\frac{\partial}{\partial w}J(w,b)$
- $ b:= b-\alpha\frac{\partial}{\partial b}J(w,b)$

### **通过计算图求导数**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E9%80%9A%E8%BF%87%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%B1%82%E5%AF%BC%E6%95%B0.JPG" width="60%" height="50%">
### **逻辑回归中的梯度下降法**
逻辑回归总结:
- $z=w^Tx+b$
- $\hat{y}=a=\sigma(z)$
- $L(a,y)=-[y\log(a)+(1-y)\log(1-a)]$

逻辑回归求导（通过计算图）:
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%B1%82%E5%AF%BC.JPG" width="60%" height="50%">
- $d_a=\frac{d}{d_a}L(a,y)=-\frac{y}{a}+\frac{1-y}{1-a}$
- $d_z=\frac{d}{d_z}L(a,y)= d_a \cdot \frac{d_a}{d_z}=(-\frac{y}{a}+\frac{1-y}{1-a}) \cdot a(1-a)=a-y$
- $d_{w_1}=\frac{\partial L}{\partial w_1}=d_z \cdot \frac{\partial z}{\partial w_1}=d_z \cdot x_1$
- $d_{w_2}=\frac{\partial L}{\partial w_2}=d_z \cdot \frac{\partial z}{\partial w_2}=d_z \cdot x_2$
- $d_{b}=\frac{\partial L}{\partial b}=d_z \cdot \frac{\partial z}{\partial b}=d_z$

### **m个样本下的梯度下降**
因为：$J(w,b)= \frac{1}{m} \sum\limits_{i=1}^m L(a^{(i)},y^{(i)})$

故:
$$\frac{\partial}{\partial w_1}J(w,b)=\frac{1}{m} \sum\limits_{i=1}^m \frac{\partial}{\partial w_1}L(a^{(i)},y^{(i)}) = \frac{1}{m} \sum\limits_{i=1}^m {d_{w_1}}^{(i)}=\frac{1}{m} \sum\limits_{i=1}^m {d_z}^{(i)} x^{(i)}_1$$
$$\frac{\partial}{\partial w_2}J(w,b)=\frac{1}{m} \sum\limits_{i=1}^m \frac{\partial}{\partial w_2}L(a^{(i)},y^{(i)})= \frac{1}{m} \sum\limits_{i=1}^m {d_{w_2}}^{(i)} =\frac{1}{m} \sum\limits_{i=1}^m {d_z}^{(i)} x^{(i)}_2$$
$$\frac{\partial}{\partial b}J(w,b)=\frac{1}{m} \sum\limits_{i=1}^m \frac{\partial}{\partial b}L(a^{(i)},y^{(i)})= \frac{1}{m} \sum\limits_{i=1}^m {d_{b}}^{(i)} =\frac{1}{m} \sum\limits_{i=1}^m {d_z}^{(i)}$$

## **2.2 python和向量化**
### **向量化**
神经网络编程指南：**无论何时，避免显式的for循环**
举例：
```Python
# 若a,b,c为向量或矩阵
c = np.dot(a,b)
b = np.exp(a)
b = np.log(a)
b = np.abs(a)
b = np.maximum(a,0)
b = a**2
b = 1/a
```
### **向量化逻辑回归**
- $X_{(n_x*m)}=[x^{(1)},x^{(2)},...,x^{(m)}]$
- $ Z_{(1*m)}=[z^{(1)},z^{(2)},...,z^{(m)}]=w^T X+ [b,...,b]$
对应代码：`Z=np.dot(w.T,X)+b #broadcasting`
- $A=[a^{(1)},a^{(2)},...,a^{(m)}]=[\sigma(z^{(1)}),\sigma(z^{(2)}),...,\sigma(z^{(m)})]=\sigma(Z)$
对应代码：`A=sigmoid(Z)`

### **向量化逻辑回归中的梯度计算**

- ${d_{z}}^{(i)}=a^{(i)}-y^{(i)}$
- 故$$d_{Z_{(1*m)}}=[{d_{z}}^{(1)},{d_{z}}^{(2)},...,{d_{z}}^{(m)}]$$
- $A=[a^{(1)},a^{(2)},...,a^{(m)}],Y=[y^{(1)},y^{(2)},...,y^{(m)}]$
- 故$$d_{Z_{(1*m)}}=A-Y=[a^{(1)}-y^{(1)}, a^{(2)}-y^{(2)},..., a^{(m)}-y^{(m)}]$$
- $$d_b=\frac{1}{m}\sum_{i=1}^{m} {d_z}^{(i)}=\frac{1}{m}np.sum(d_Z)$$
- $$d_w= \begin{bmatrix} d_{w_1} \\d_{w_2} \\ ... \\d_{w_{n_x}} \end{bmatrix} = \begin{bmatrix} \frac{1}{m} \sum\limits_{i=1}^m {d_z}^{(i)} x^{(i)}_1 \\\frac{1}{m} \sum\limits_{i=1}^m {d_z}^{(i)} x^{(i)}_2 \\ ... \\\frac{1}{m} \sum\limits_{i=1}^m {d_z}^{(i)} x^{(i)}_{n_x} \end{bmatrix} \\
=\frac{1}{m} X {d_{Z}}^T 
=\frac{1}{m}[x^{(1)} ,x^{(2)},...,x^{(m)}] \begin{bmatrix} {d_{z}}^{(1)} \\ {d_{z}}^{(2)}\\ ...\\{d_{z}}^{(m)}\end{bmatrix}$$

### **逻辑回归算法流程（伪代码）**
非向量化：
```
J=0, dw1=0, dw2=0,db=0
for i = 1 to m
	z(i) = wx(i)+b
	a(i) = sigmoid(z(i))
	J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i))
	dz(i) = a(i)-y(i)
	dw1 += x1(i)dz(i)
	dw2 += x2(i)dz(i)
	db += dz(i)
J /= m
dw1 /= m
dw2 /= m
db /= m
```
向量化：
```
for iter in range(1000):
    Z = np.dot(w.T,X) + b
    A = sigmoid(Z)
    dZ = A-Y
    dw = 1/m*np.dot(X,dZ.T)
    db = 1/m*np.sum(dZ)
    w = w - alpha*dw
    b = b - alpha*db
```
### **python中的广播**
广播(broadcasting)是Python使用中的一种技巧，在Python中可以对不同维度的矩阵进行四则混合运算，前提条件是至少有一个维度是相同的。
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/python%E5%B9%BF%E6%92%AD1.jpg" width="50%" height="50%"> <img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/python%E5%B9%BF%E6%92%AD2.JPG" width="50%" height="50%">

### **关于python/numpy中的向量说明**
Python中，如果用下列语句来定义一个向量：
```
a = np.random.randn(5)
```
这条语句生成的$a$的维度是$(5,)$。它既不是行向量也不是列向量，我们把$a$叫做"rank1 array"。这种定义会带来一些问题。例如我们对$a$进行转置，还是会得到$a$本身。
所以，如果我们要定义$(5,1)$的列向量或者$(1,5)$的行向量，最好使用下来标准语句，避免使用"rank1 array"。
```
a = np.random.randn(5,1)
b = np.random.randn(1,5)
```
除此之外，我们还可以使用`assert`语句对向量或数组的维度进行判断，例如：
```
assert(a.shape == (5,1))
```
`assert`会对内嵌语句进行判断，即判断$a$的维度是不是$(5,1)$的。如果不是，则程序在此处停止。使用`assert`语句也是一种很好的习惯，能够帮助我们及时检查、发现语句是否正确。
另外，还可以使用`reshape`函数对数组设定所需的维度：
```
a = a.reshape((5,1))
```

# **3.浅层神经网络**
## **神经网络表示**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA.JPG" width="60%" height="50%">以上图为例，注意以下表示：
- 输入层（第0层），隐藏层（第1层），输出层（第2层），是一个两层的神经网络。
- $a^{[1]}$表示第一层的激活函数(输出)值,$a^{[1]}=[a^{[1]}_1,a^{[1]}_2,a^{[1]}_3,a^{[1]}_4]^T$,其维度为`$n^{[1]}*1$`，即`$4*1$`。
- $W^{[1]}$表示第一层的权重值,其维度为`$n^{[1]}*n^{[0]}$`，即`$4*3$`。(注意)第$l$层的权重$W^{[l]}$的维度为：`$n^{[l]}*n^{[l-1]}$`。
- $b^{[1]}$表示第一层的偏置值,其维度为`$n^{[1]}*1$`，即`$4*1$`。
- 预测值$\hat{y}$即为神经网络最后一层的激活函数值$a^{[2]}$，即$\hat{y}=a^{[2]}$。

## **计算神经网络的输出**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BE%93%E5%87%BA1.JPG" width="50%" height="50%">首先，神经网络中的每一个神经元，都可以看成一个逻辑回归单元，经过两个步骤来计算输出值$a$:
- $z=w^Tx+b$
- $a = \sigma(z)$


<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BE%93%E5%87%BA2.JPG" width="50%" height="50%">对于神经网络的输出，可向量化为（可通过维度检查进行确认）：
- $z^{[1]}=W^{[1]}a^{[0]}+b^{[1]}$
- $a^{[1]}=\sigma(z^{[1]})$
- $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$
- $a^{[2]}=\sigma(z^{[2]})$

## **m个样本中的向量化**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E5%A4%9A%E6%A0%B7%E6%9C%AC%E4%B8%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96.jpg" width="50%" height="50%">多个样本的情况下，向量化神经网络输出的方式为：
- $Z^{[1]}=W^{[1]}X+b^{[1]}$
- $A^{[1]}=\sigma(Z^{[1]})$
- $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
- $A^{[2]}=\sigma(Z^{[2]})$
- **该向量化方式的核心思想是**：$X$,$Z^{[i]}$及$A^{[i]}$的横向维度表示训练样本，纵向维度表示隐藏单元。形象化示意见图。

## **激活函数**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.jpg" width="50%" height="50%">
- `sigmoid`函数和`tanh`函数都有一个问题，就是当$z$很大或很小时，激活函数的梯度接近于0，会拖慢梯度下降算法。
- 研究表明，当隐藏层使用`tanh`函数几乎总比`sigmoid`函数的表现更好。因为`tanh`函数的激活函数值范在[-1,1]之间，激活函数的平均值接近于0，更方便下一层的学习。
- 如果是二分类问题，即样本标签$y$为0或1，希望预测值$\hat{y}$取值为$0\leq\hat{y}\leq1$，则输出层的激活函数可选用`sigmoid`函数，**否则隐藏层和输出层都不使用`sigmoid`函数**。
- `ReLU(rectified  linear unit)`激活函数在$z$大于零时梯度始终为1，在$z$小于零时梯度始终为0，$z$等于零时的梯度无定义，可以当成1也可以当成0，实际应用中并不影响（因为在程序中值为0.000...的可能性很小）。**当前隐藏层的默认激活函数选择为`ReLU`**。
- `ReLU`激活函数能够保证$z$大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度（虽然左半边函数的导数等于0，但有足够多的隐藏单元使$z$大于0，因此大部分样本都会训练地很快）。
- `ReLU`激活函数的缺点为当$z$小于零时，导数为0，这个缺点在实际应用中没什么影响。为了弥补这个缺点，出现了`Leaky ReLU`激活函数，能够保证$z$小于零时梯度不为0，不过实际中使用的频率不高。

## **为什么需要非线性激活函数**
以上的四种激活函数都是非线性的。不可使用线性的激活函数，原因如下。
假设所有的激活函数都是线性的，那么，浅层神经网络的各层输出为：
- $z^{[1]}=w^{[1]}x+b^{[1]}$
- $a^{[1]}=g^{[1]}(z^{[1]})=z^{[1]}$
- $z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}$
- $a^{[2]}=g^{[2]}(z^{[2]})=z^{[2]}$

对$a^{[2]}$进行化简:
$$a^{[2]}=z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}\\
=w^{[2]}(w^{[1]}x+b^{[1]})+b^{[2]}\\
=(w^{[2]}w^{[1]})x+(w^{[2]}b^{[1]}+b^{[2]})\\
=w^{'}+b^{'}$$
- 经过推导发现$a^{[2]}$仍是输入变量$x$的线性组合。这表明，即便是包含多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入$x$的线性模型。
- 因此，**隐藏层的激活函数必须要是非线性的。线性隐藏层没有任何作用，层数再多也不行**。
- 只有一种情况可以使用线性激活函数，即对于回归问题，当输出$\hat{y}$是一个实数时，**输出层的激活函数可以使用线性函数**。如果输出$\hat{y}$是非负数，则可以使用`ReLU`激活函数。具体情况，具体分析。

## **激活函数的导数**
sigmoid函数的导数：
- $g(z)=\frac{1}{1+e^{(-z)}}$
- $g'(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)$

tanh函数的导数：
- $g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}}$
- $g'(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2$

ReLU函数的导数：
- $g(z)=max(0,z)$
- $$g'(z)=\begin{cases} 0, & z<0\\  1, & z\geq0 \end{cases}$$

Leaky ReLU函数的导数：
- $g(z)=max(0.01z,z)$
- $$g'(z)=\begin{cases} 0.01, & z<0\\ 1, & z\geq0 \end{cases}$$

## **神经网络的梯度下降法**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D1.JPG" width="50%" height="50%"> <img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D2.JPG" width="50%" height="50%">单样本神经网络正向传播过程为：
- $z^{[1]}=W^{[1]}x+b^{[1]}$
- $a^{[1]}=g(z^{[1]})$
- $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$
- $a^{[2]}=g(z^{[2]})$
- 其中，$g(\cdot)$表示激活函数。

单样本神经网络反向传播过程（即链式法则求导过程）：
- $da^{[2]}=-\frac{y}{a}+\frac{1-y}{1-a}$（不同损失函数，结果不同，可不给出具体形式）
- $dz^{[2]}=a^{[2]}-y$（不同损失函数，结果不同，可不给出具体形式，一般只用$d_z$，不用$d_a$）
- $dW^{[2]}=dz^{[2]}a^{[1]T}$
- $db^{[2]}=dz^{[2]}$
- $da^{[1]}=W^{[2]T}dz^{[2]}$
- $dz^{[1]}=W^{[2]T}dz^{[2]}\ast g^{[1]'}(z^{[1]})$
- $dW^{[1]}=dz^{[1]}x^T$
- $db^{[1]}=dz^{[1]}$
- 其中，$*$为逐元素相乘。

<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D3.JPG" width="50%" height="50%">$m$个样本神经网络正向传播过程为：
- $Z^{[1]}=W^{[1]}X+b^{[1]}$
- $A^{[1]}=g(Z^{[1]})$
- $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
- $A^{[2]}=g(Z^{[2]})$
- 其中，$g(\cdot)$表示激活函数。

$m$个样本神经网络反向传播过程（即$m$个单样本的梯度求和）：
- $dZ^{[2]}=A^{[2]}-Y$
- $dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T}$
- $db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdims=True)$
- $dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g^{[1]'}(Z^{[1]})$
- $dW^{[1]}=\frac1mdZ^{[1]}X^T$
- $db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdims=True)$
- 其中，$*$为逐元素相乘。

该节的一个重要参考：[神经网络反向传播的数学原理](https://zhuanlan.zhihu.com/p/22473137)

## **随机初始化**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96.jpg" width="50%" height="50%">
神经网络模型中的参数权重$W$不能全部初始化为零。
举例说明，如图，如果权重$W^{[1]}$和$W^{[2]}$都初始化为零，即：
$$W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right]$$
$$W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]$$

- 这样使得$a_1^{[1]}=a_2^{[1]}$。进一步可得$dz_1^{[1]}=dz_2^{[1]}$ ，以及$dW_1^{[1]}=dW_2^{[1]}$。
- 因此，每次迭代更新：$W^{[1]} = W^{[1]}-dW^{[1]}$。 $W^{[1]}$每一行都相等，即$W_1^{[1]}$和$W_2^{[1]}$都会相等。无论经过多少次迭代，隐藏层的神经元总是对称的。这样隐藏层设置多个神经元就没有任何意义了。
- 参数$b$全部初始化为零，不会产生对称失效问题。
- 解决方法：将$W$进行随机初始化($b$可初始化为零)来打破对称(break symmetry)。

Python中可以使用如下语句进行$W$和$b$的随机初始化：
```
W_1 = np.random.randn(2,2)*0.01
b_1 = np.zeros((2,1))
W_2 = np.random.randn(1,2)*0.01
b_2 = 0
```
- 在对$W^{[1]}$进行随机初始化时，乘以0.01的目的是尽量使得权重$W$初始化为比较小的值。
- 之所以让$W$比较小，是因为如果使用`sigmoid`函数或者`tanh`函数作为激活函数时，若$W$较大，则训练的开始阶段$z$就比较大，由`sigmoid`函数或者`tanh`函数的曲线可以看出，当$|z|$过大时，其梯度近似为0，会使得训练过程十分缓慢。
- 当然，如果未使用`sigmoid`激活函数或者`tanh`激活函数，该情况可能不明显。但是如果对于二分类问题，输出层是`sigmoid`函数，则对应的权重$W$最好初始化到比较小的值。

# **4.深层神经网络**
## **深层神经网络**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.JPG" width="70%" height="50%">深层神经网络其实就是包含更多隐藏层的神经网络。如上图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和5个隐藏层的神经网络它们的模型结构。
## **深层网络中的前向传播**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD.JPG" width="50%" height="50%">以上面讲过的4层神经网络为例，推导一下深层神经网络的正向传播过程。
单个样本下，深层神经网络的正向传播过程：
- $l=1$：$z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}$，$a^{[1]}=g^{[1]}(z^{[1]})$
- $l=2$：$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$，$a^{[2]}=g^{[2]}(z^{[2]})$
- $l=3$：$z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}$，$a^{[3]}=g^{[3]}(z^{[3]})$
- $l=4$：$z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}$，$a^{[4]}=g^{[4]}(z^{[4]})$

$m$个样本下，深层神经网络的正向传播过程：
- $l=1$：$Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$，$A^{[1]}=g^{[1]}(Z^{[1]})$
- $l=2$：$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$，$A^{[2]}=g^{[2]}(Z^{[2]})$
- $l=3$：$Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}$，$A^{[3]}=g^{[3]}(Z^{[3]})$
- $l=4$：$Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}$，$A^{[4]}=g^{[4]}(Z^{[4]})$

综上所述，对于第$l$层，其正向传播过程的$Z^{[l]}$和$A^{[l]}$可以表示为：
- $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$
- $A^{[l]}=g^{[l]}(Z^{[l]})$
- 其中，$l=1,\cdots,L$

## **矩阵维度检查**
对于单个训练样本，输入$x$的维度是:
- $x:(n^{[0]},1)$。

神经网络的参数$W^{[l]}$和$b^{[l]}$的维度分别是：
- $W^{[l]}:\ (n^{[l]},n^{[l-1]})$
- $b^{[l]}:\ (n^{[l]},1)$
- 其中，$l=1,\cdots,L$，$n^{[l]}$和$n^{[l-1]}$分别表示第$l$层和$l-1$层中所含神经元的个数。$n^{[0]}=n_x$，表示输入尺寸。

正向传播过程中的$z^{[l]}$和$a^{[l]}$的维度分别是：
- $z^{[l]}:\ (n^{[l]},1)$
- $a^{[l]}:\ (n^{[l]},1)$
- $z^{[l]}$和$a^{[l]}$的维度是一样的，且$dz^{[l]}$和$da^{[l]}$的维度均与$z^{[l]}$和$a^{[l]}$的维度一致。

反向传播过程中的$dW^{[l]}$和$db^{[l]}$的维度分别是：
- $dW^{[l]}:\ (n^{[l]},n^{[l-1]})$
- $db^{[l]}:\ (n^{[l]},1)$
- 注意到， $dW^{[l]}$与$W^{[l]}$的维度相同，$db^{[l]}$与$b^{[l]}$的维度相同。



对于$m$个训练样本，输入矩阵$X$的维度是：
- $X:(n^{[0]},m)$。

参数$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：
- $W^{[l]}:\ (n^{[l]},n^{[l-1]})$
- $b^{[l]}:\ (n^{[l]},1)$
- 只不过在运算$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$中，$b^{[l]}$会被当成$(n^{[l]},m)$矩阵进行运算，这是因为python的广播性质，且 $b^{[l]}$每一列向量都是一样的。
- $dW^{[l]}$和$db^{[l]}$的维度分别与$W^{[l]}$和$b^{[l]}$一致。

但是，$Z^{[l]}$ 和$A^{[l]}$的维度发生了变化：
- $Z^{[l]}:\ (n^{[l]},m)$
- $A^{[l]}:\ (n^{[l]},m)$
- $dZ^{[l]}$和$dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$一致。

## **为什么使用深层的神经网络**
- 神经网络层数越深，能够提取到的特征越复杂。
- 以CNN为例，低层网络提取到简单的局部特征，深层网络提取到复杂的全局特征。层数越深，提取到的特征越复杂。
- 以RNN为例，也是如此。

## **搭建深层神经网络块**
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97.jpg" width="50%" height="50%">
上图为能表示出深层神经网络正向传播和反向传播过程的块(block)图。
如图所示，对于第$l$层来说，正向传播过程中：
- 输入：$a^{[l-1]}$
- 输出：$a^{[l]}$
- 参数：$W^{[l]},b^{[l]}$
- 缓存变量： $z^{[l]}$

反向传播过程中：
- 输入：$da^{[l]}$
- 输出：$da^{[l-1]},dW^{[l]},db^{[l]}$
- 参数：$W^{[l]},b^{[l]}$

用块图构建出一个深层神经网络的正向传播过程和反向传播过程，如下图所示：
<img src="http://ozruihqgo.bkt.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%BE%E7%A8%8B1/l%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97%E5%9B%BE.jpg" width="100%" height="50%">

## **正向传播和反向传播**

对于单个样本，第$l$层的正向传播：
- 输入：$a^{[l-1]}$
- 输出：$a^{[l]}$，缓存变量：$z^{[l]}$

具体表达式如下：
- $z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$
- $a^{[l]}=g^{[l]}(z^{[l]})$

$m$个训练样本，向量化形式为：
- $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$
- $A^{[l]}=g^{[l]}(Z^{[l]})$

对于单个样本，第$l$层的反向传播：
- 输入：$da^{[l]}$
- 输出：$da^{[l-1]}$, $dW^{[l]}$, $db^{[l]}$。

具体表达式如下：
- $dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]})$
- $dW^{[l]}=dz^{[l]}\cdot a^{[l-1]}$
- $db^{[l]}=dz^{[l]}$
- $da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}$
- 由上述第四个表达式可得$da^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}$，将$da^{[l]}$代入第一个表达式中可以得到：$dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})$该式非常重要，反映了$dz^{[l+1]}$与$dz^{[l]}$的递推关系。

$m$个训练样本，向量化形式为：
- $dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]})$
- $dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T}$
- $db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdims=True)$
- $dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}$
- $dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]})$

## **参数和超参数**
对于神经网络中的参数(parameters)和超参数(hyperparameters)的概念：
- 神经网络中的参数就是我们熟悉的$W^{[l]}$和$b^{[l]}$。
- 而超参数则是例如学习速率$\alpha$，训练迭代次数$N$，神经网络层数$L$，各层神经元个数$n^{[l]}$，甚至激活函数$g(z)$的种类等。
- 之所以**叫做超参数的原因**是它们高于参数，决定了参数$W^{[l]}$和$b^{[l]}$的取值。


