---
title: 线性回归模型
mathjax: true
top: true
date: 2017-12-3 15:35:50
categories: 
- 机器学习
tags:
- 线性回归
- linear regression
- 似然估计
- 梯度下降
- 正规方程
- 正则化
---
线性回归(linear regression)属于监督学习(supervised learning)中的回归(regression)问题，模型输出为连续值。
## 1.模型
### 1.1 假设函数
假设函数：`$h_\theta(x)=\sum\limits_{i=0}^n\theta_i x_i=\theta^Tx$`
其中，$x=[x_0,x_1,...,x_n]^T\in R^{n+1}$, $x_0^{(i)}=1$(对应偏置项); $\theta = [\theta_0,\theta_1,...,\theta_n]^T\in R^{n+1}$
### 1.2 代价函数
对于训练集：`$\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)},...,(x^{(m)},y^{(m)})\}$`，其中$y^{(i)}\in R$
代价函数： `$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$`
<!-- more --> 
### 1.3 参数求解
目的：$\min\limits_\theta \ J(\theta)$
#### 1.3.1 梯度下降法
$$\begin{split}
\theta_j : &=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\\
&=2\cdot\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x^{(i)})-y^{(i)})\\
&=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot\frac{\partial}{\partial\theta_j}(\sum_{k=1}^n\theta_kx^{(i)}_k-y^{(i)})\\
&=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
\end{split}
$$ 
注1：对于每一轮迭代，进行参数更新时，对参数$\theta$中的$j=0,1,...,n$项同时更新。
注2：线性回归的代价函数$J(\theta)$没有局部最小值，只有全局最小值。

#### 1.3.2 梯度下降在实际应用中的技巧
（1）特征缩放
- 进行特征缩放，确保所有特征在一个相似的尺度内，可以加速梯度下降，避免因为某一个或多个特征远大于其他特征而造成的许多额外的迭代。
- 特征缩放：$x_1 = \frac{x_1-\mu_1}{s_1}$，$\mu_1$为均值，$s_1$为方差或标准差或特征范围(特征中最大值减最小值)。
- 建议：让每一个特征缩放到$-1\leq x_i \leq 1(x_0=1)$，上限为$-3$~$3$，下限为$\frac{1}{3}$~$\frac{1}{3}$。

（2）学习率
- 对于调试：对于足够小的学习率$\alpha$，$J(\theta)$应该每一次迭代都减小；太小的$\alpha$会导致收敛过慢；太大的$\alpha$会导致$J(\theta)$不减小或不收敛。
- 对于学习率选择：确定一个不能再小的$\alpha_0$，确定一个不能再大的$\alpha_1$，选择比$\alpha_1$小一点的尽量大的$\alpha$。
建议：尝试 $...,0.001,0.003,0.01,0.03,0.1,0.3,1,...$

#### 1.3.3 正规方程
$$
\begin{eqnarray*}
J(\theta)&=&\frac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
&=&\frac{1}{2}(y-X\theta)^T(y-X\theta)
\end{eqnarray*}$$
式中，`$X_{m\times(n+1)}= \begin{bmatrix} {x^{(1)}}^T \\ {x^{(2)}}^T \\ \vdots\\ {x^{(m)}}^T\end{bmatrix}$`，`$y_{m\times 1}= \begin{bmatrix} {y^{(1)}} \\ {y^{(2)}} \\ \vdots\\ {y^{(m)}}\end{bmatrix}$`，`$\theta_{(n+1)\times 1}=\begin{bmatrix} \theta_0 \\  \theta_1 \\ \vdots\\  \theta_n \end{bmatrix}$`，`$x_{(n+1)\times 1}=\begin{bmatrix} x_0 \\  x_1 \\ \vdots\\  x_n \end{bmatrix}$`
矩阵微分中有如下结论：$\frac{\partial A\theta}{\partial\theta}=A^T$和$\frac{\partial\theta^TA\theta}{\partial\theta}=2A^T\theta$。
$$
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial\theta}&=&\frac{\partial}{\partial\theta}(y^Ty+\theta^TX^TX\theta-2y^TX\theta)\\
&=&2X^TX\theta-2X^Ty
\end{eqnarray*}$$
上式中$y^TX\theta$是标量，令$\frac{\partial J(\theta)}{\partial\theta}=0$，得：
$$X^TX\theta=X^Ty$$
上式也称为正规方程。
$$\theta=(X^TX)^{-1}X^Ty$$
注：$X^TX$是可逆的，只要$X^TX$的列线性无关。
## 2.概率解释
1.假设每一个标签$y^{(i)}$符合均值为$\theta^Tx^{(i)}$及方差为$\sigma^2$的高斯分布：
$$y^{(i)}=N(\theta^Tx^{(i)},\sigma^2)=\theta^Tx^{(i)}+N(0,\sigma^2)$$
2.已知输入及参数，预测值的概率分步如下：
$$P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
3.似然函数如下：
$$
\begin{eqnarray*}
L(\theta)&=&\prod\limits_{i=1}^mP(y^{(i)}|x^{(i)},\theta)\\
&=&\prod\limits_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{eqnarray*}
$$
4.似然函数的$log$形式如下：
$$\begin{eqnarray*}
l(\theta)&=&logL(\theta)\\
&=&log\prod\limits_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
&=&\sum\limits_{i=1}^mlog\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
&=&mlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}\cdot\frac{1}{2}\sum\limits_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2
\end{eqnarray*}$$
最大化似然函数$l(\theta)$等价于最小化代价函数`$J(\theta)=\frac{1}{2}\sum\limits_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$`
注：一维正态分布PDF：$f(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$ 
对数函数和差性质：`$log_\alpha AB=log_\alpha A+log_\alpha B$`；`$log_\alpha\frac{A}{B}=log_\alpha A-log_\alpha B$`
## 3.正则化
### 3.1 岭回归（L2范数正则化）
$$\min_{\theta}\frac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda||\theta||^2_2$$
式中，`$||\theta||^2_2=\sum\limits_{j=1}^n\theta^2_j=\theta^T\theta$`
### 3.2 lasso回归(L1范数正则化)
$$\min_{\theta}\frac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda||\theta||_1$$
式中，`$||\theta||_1=\sum\limits_{j=1}^n|\theta_j|$`
注1:
- L0：计算非零个数，用于产生稀疏性，但是在实际研究中很少用，因为L0范数很难优化求解，是一个NP-hard问题，因此更多情况下使用L1范数近似；
- L1：计算绝对值之和，用以产生稀疏性，因为它是L0范式的一个最优凸近似，容易优化求解；
- L2：计算平方和再开根号，L2范数更多是防止过拟合，并且让优化求解变得稳定很快速（这是因为加入了L2范式之后，满足了强凸）。

注2：对于向量范数：
- p范数：`$||x||_p=(\sum\limits_{i=1}^n |x_i|^p)^{\frac{1}{p}}$`
- L1范数：`$||x||_1=\sum\limits_{i=1}^n |x_i|$`
- L2范数：`$||x||_2=\sqrt{\sum\limits_{i=1}^n x^2_i}$`

## 4.参考资料
- [ Andrew Ng, Machine Learning](https://www.coursera.org/learn/machine-learning)
- [林轩田，机器学习基石](https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf)
- [Nando de Freitas, CPS540](https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6)
- 周志华，机器学习



